# 技术架构文档

## 系统架构概览

RustSentinel 采用分层架构设计，实现了从用户交互到硬件加速的完整技术栈。

```
┌─────────────────────────────────────────────────────────┐
│                    用户交互层                            │
│           Gradio Web UI / Streamlit UI                   │
└─────────────────────┬───────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────┐
│                  应用逻辑层                              │
│        Prompt 工程 / 业务逻辑 / API 封装                 │
└─────────────────────┬───────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────┐
│                  推理引擎层                              │
│     vLLM Server (OpenAI Compatible API)                 │
│     DeepSeek-R1-Distill-Qwen-7B Model                   │
└─────────────────────┬───────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────┐
│                  硬件加速层                              │
│          沐曦 C500 GPU / NVIDIA GPU                      │
│              CUDA / ROCm Runtime                         │
└─────────────────────────────────────────────────────────┘
```

## 核心组件

### 1. 用户交互层

**技术栈**: Gradio 4.x / Streamlit 1.28+

**职责**:
- 提供 Web 可视化界面
- 接收用户输入的 Rust 代码
- 实时流式展示审计结果
- 支持代码高亮和 Markdown 渲染

**关键文件**:
- `src/app_gradio.py` - Gradio 主界面
- `src/app.py` - Streamlit 主界面（备选）

### 2. 应用逻辑层

**技术栈**: Python 3.10+ / OpenAI SDK

**职责**:
- Prompt 工程和模板管理
- 业务逻辑封装
- 错误处理和日志记录
- API 请求路由

**核心模块**:

```python
# Prompt 模板
SYSTEM_PROMPT = """
你是一个资深的 Solana 智能合约安全审计专家。
请仔细分析代码逻辑，重点检测：
1. 权限绕过漏洞（Signer Check 缺失）
2. 账户所有权验证缺失
3. 整数溢出/下溢
4. 重入攻击风险
5. 逻辑错误

输出格式：
## 风险等级
[严重/高危/中危/低危]

## 漏洞描述
[详细描述发现的问题]

## 攻击原理
[说明如何利用该漏洞]

## 修复建议
[提供修复后的代码]
"""
```

### 3. 推理引擎层

**技术栈**: vLLM / DeepSeek-R1-Distill

**职责**:
- 高性能模型推理
- 内存优化和批处理
- OpenAI 兼容 API 服务
- 流式输出支持

**配置示例**:

```bash
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --dtype bfloat16 \                  # 使用 BF16 精度
    --gpu-memory-utilization 0.9 \      # GPU 显存利用率
    --max-model-len 8192 \              # 最大上下文长度
    --port 8000 \                       # 服务端口
    --model-name deepseek-audit         # 模型别名
```

### 4. 硬件加速层

**支持硬件**:
- ✅ 沐曦 C500 GPU (64GB 显存)
- ✅ NVIDIA A100/H100
- ✅ NVIDIA RTX 4090/3090

**性能指标**:
- **推理延迟**: < 2 秒（单次审计）
- **吞吐量**: ~100 tokens/秒
- **显存占用**: ~18GB（BF16 模式）

## 数据流

### 完整审计流程

```
用户输入代码
    │
    ▼
前端收集输入
    │
    ▼
构造 API 请求 {
    model: "deepseek-audit",
    messages: [
        {role: "system", content: SYSTEM_PROMPT},
        {role: "user", content: user_code}
    ]
}
    │
    ▼
HTTP POST → localhost:8000/v1/chat/completions
    │
    ▼
vLLM 接收请求
    │
    ▼
加载模型到 GPU
    │
    ▼
Token 化处理
    │
    ▼
模型推理（GPU 加速）
    │
    ▼
流式生成响应
    │
    ▼
返回审计报告（Markdown）
    │
    ▼
前端实时渲染
    │
    ▼
用户查看结果
```

## 安全设计

### 隐私保护机制

1. **本地化部署**: 所有推理在本地 GPU 完成，代码不离开本地网络
2. **无日志存储**: 默认不记录用户输入的代码
3. **内存即时释放**: 推理完成后立即清除内存中的代码
4. **离线运行**: 支持完全离线环境下运行

### 网络隔离

```
┌──────────────┐
│  用户浏览器   │
└──────┬───────┘
       │ HTTP (localhost only)
┌──────▼───────┐
│ Gradio Server │
│  (port 8501)  │
└──────┬───────┘
       │ HTTP (localhost only)
┌──────▼───────┐
│  vLLM Server  │
│  (port 8000)  │
└──────┬───────┘
       │ GPU Compute
┌──────▼───────┐
│   C500 GPU   │
└──────────────┘
```

所有通信仅在 `localhost` 环回接口，外部网络无法访问。

## 性能优化

### GPU 显存优化

1. **BF16 量化**: 减少 50% 显存占用，性能损失 < 1%
2. **PagedAttention**: vLLM 的显存管理技术，提高并发能力
3. **动态批处理**: 自动合并多个请求以提高吞吐量

### 推理加速

1. **Flash Attention 2**: 加速注意力机制计算
2. **Continuous Batching**: 连续批处理技术
3. **KV Cache**: 缓存键值对减少重复计算

### 并发处理

```python
# vLLM 自动管理并发
# 支持多个用户同时审计
max_concurrent_requests = 32  # 可配置
```

## 可扩展性

### 水平扩展

使用负载均衡器分发请求到多个 vLLM 实例：

```
                  ┌─────────────┐
                  │Load Balancer│
                  └──────┬──────┘
         ┌───────────────┼───────────────┐
         │               │               │
    ┌────▼────┐    ┌────▼────┐    ┌────▼────┐
    │ vLLM-1  │    │ vLLM-2  │    │ vLLM-3  │
    │ C500-1  │    │ C500-2  │    │ C500-3  │
    └─────────┘    └─────────┘    └─────────┘
```

### 垂直扩展

利用张量并行跨多 GPU 部署：

```bash
vllm serve ... --tensor-parallel-size 2  # 使用 2 个 GPU
```

## 监控和日志

### 日志级别

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### 关键指标

- 请求延迟（P50, P95, P99）
- GPU 利用率
- 显存使用率
- 请求成功率
- 并发用户数

### 监控工具

推荐使用 Prometheus + Grafana 监控 vLLM 服务。

## 技术选型理由

| 技术 | 选型理由 |
|------|---------|
| **Gradio** | 快速构建 ML 应用界面，支持流式输出，社区活跃 |
| **vLLM** | 业界最快的 LLM 推理引擎，显存效率高，支持 OpenAI API |
| **DeepSeek-R1** | 强大的逻辑推理能力，适合代码分析，开源可商用 |
| **Python** | 丰富的 AI/ML 生态，开发效率高 |
| **Docker** | 简化部署，环境一致性 |

## 未来规划

### 短期优化（1-3个月）

- [ ] 集成 RAG 技术，挂载 Solana 官方文档
- [ ] 支持批量文件审计
- [ ] 优化 Prompt 模板，提高检测准确率
- [ ] 添加审计报告导出功能（PDF/HTML）

### 中期规划（3-6个月）

- [ ] 支持更多区块链语言（Move, Cairo）
- [ ] 开发 VS Code 插件
- [ ] 构建漏洞知识库
- [ ] 集成 CI/CD 流水线

### 长期愿景（6-12个月）

- [ ] 多模态支持（代码 + 架构图分析）
- [ ] 联邦学习增强模型
- [ ] 自动化修复代码
- [ ] 社区驱动的规则库

---

更多技术细节请参考源码注释和 API 文档。

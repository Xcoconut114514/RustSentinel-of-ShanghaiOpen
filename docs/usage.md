# ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£æä¾› RustSentinel çš„è¯¦ç»†ä½¿ç”¨è¯´æ˜ã€‚

## ğŸš€ å¯åŠ¨æœåŠ¡

### æ–¹æ³•ä¸€ï¼šåˆ†æ­¥å¯åŠ¨ï¼ˆæ¨èç”¨äºå¼€å‘ï¼‰

#### 1. å¯åŠ¨ vLLM æ¨ç†æœåŠ¡

åœ¨ç¬¬ä¸€ä¸ªç»ˆç«¯ä¸­ï¼š

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å¯åŠ¨ vLLM
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --dtype bfloat16 \
    --port 8000 \
    --gpu-memory-utilization 0.9 \
    --model-name deepseek-audit
```

ç­‰å¾…çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

#### 2. å¯åŠ¨ Web ç•Œé¢

åœ¨ç¬¬äºŒä¸ªç»ˆç«¯ä¸­ï¼š

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å¯åŠ¨ Gradio ç•Œé¢
cd src
python app_gradio.py
```

è®¿é—® http://localhost:8501 å¼€å§‹ä½¿ç”¨ã€‚

### æ–¹æ³•äºŒï¼šä½¿ç”¨ Docker

```bash
docker-compose -f docker/docker-compose.yml up -d
```

è¯¦ç»†è¯´æ˜è§ [Docker éƒ¨ç½²æ–‡æ¡£](docker-deployment.md)ã€‚

## ğŸ’» åŸºæœ¬ä½¿ç”¨

### Web ç•Œé¢ä½¿ç”¨

1. **ç²˜è´´ä»£ç **
   - åœ¨å·¦ä¾§æ–‡æœ¬æ¡†ç²˜è´´å¾…å®¡è®¡çš„ Rust ä»£ç 
   - æˆ–è€…ç‚¹å‡»"åŠ è½½ç¤ºä¾‹"æŒ‰é’®ä½¿ç”¨é¢„è®¾ç¤ºä¾‹

2. **å¼€å§‹å®¡è®¡**
   - ç‚¹å‡»"ğŸš€ å¼€å§‹å®¡è®¡"æŒ‰é’®
   - ç­‰å¾… AI åˆ†æï¼ˆé€šå¸¸ 5-30 ç§’ï¼‰

3. **æŸ¥çœ‹æŠ¥å‘Š**
   - å³ä¾§å®æ—¶æ˜¾ç¤ºå®¡è®¡è¿›åº¦
   - ç”Ÿæˆ Markdown æ ¼å¼çš„å®¡è®¡æŠ¥å‘Š
   - åŒ…å«æ¼æ´ç­‰çº§ã€æè¿°ã€ä¿®å¤å»ºè®®

4. **å¯¼å‡ºæŠ¥å‘Š**ï¼ˆå³å°†æ”¯æŒï¼‰
   - ç‚¹å‡»"å¯¼å‡º PDF"ä¿å­˜æŠ¥å‘Š
   - æˆ–å¤åˆ¶ Markdown æ–‡æœ¬

### å‘½ä»¤è¡Œä½¿ç”¨

åˆ›å»º Python è„šæœ¬ï¼š

```python
# my_audit.py
from openai import OpenAI

# è¯»å–å¾…å®¡è®¡çš„ä»£ç 
with open("my_contract.rs", "r") as f:
    code = f.read()

# è¿æ¥æœ¬åœ°æ¨¡å‹
client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")

# æ‰§è¡Œå®¡è®¡
response = client.chat.completions.create(
    model="deepseek-audit",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ Solana æ™ºèƒ½åˆçº¦å®‰å…¨å®¡è®¡ä¸“å®¶ã€‚"
        },
        {
            "role": "user",
            "content": f"è¯·å®¡è®¡è¿™æ®µä»£ç ï¼š\n\n{code}"
        }
    ],
    temperature=0.1,
    max_tokens=2048
)

# ä¿å­˜æŠ¥å‘Š
report = response.choices[0].message.content
with open("audit_report.md", "w") as f:
    f.write(report)

print("å®¡è®¡å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ° audit_report.md")
```

è¿è¡Œï¼š

```bash
python my_audit.py
```

## ğŸ¯ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

ç¼–è¾‘ `config/system_prompt.txt` æ¥å®šåˆ¶å®¡è®¡è¡Œä¸ºï¼š

```text
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆçº¦å®‰å…¨ä¸“å®¶ï¼Œä¸“æ³¨äºï¼š

1. æƒé™ç»•è¿‡æ¼æ´
2. é‡å…¥æ”»å‡»
3. [ä½ çš„è‡ªå®šä¹‰æ£€æµ‹é¡¹]

è¾“å‡ºæ ¼å¼ï¼š
[ä½ çš„è‡ªå®šä¹‰æ ¼å¼]
```

### æ‰¹é‡å®¡è®¡

åˆ›å»ºæ‰¹é‡å®¡è®¡è„šæœ¬ï¼š

```python
# batch_audit.py
import os
from openai import OpenAI

client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")

# å®¡è®¡ç›®å½•ä¸‹æ‰€æœ‰ .rs æ–‡ä»¶
contracts_dir = "contracts/"
for filename in os.listdir(contracts_dir):
    if filename.endswith(".rs"):
        print(f"æ­£åœ¨å®¡è®¡: {filename}")
        
        with open(os.path.join(contracts_dir, filename), "r") as f:
            code = f.read()
        
        response = client.chat.completions.create(
            model="deepseek-audit",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å®‰å…¨å®¡è®¡ä¸“å®¶"},
                {"role": "user", "content": f"å®¡è®¡ä»£ç ï¼š\n{code}"}
            ]
        )
        
        # ä¿å­˜æŠ¥å‘Š
        report_name = f"report_{filename}.md"
        with open(report_name, "w") as f:
            f.write(response.choices[0].message.content)
        
        print(f"âœ“ å®Œæˆ: {report_name}\n")
```

### æµå¼è¾“å‡º

è·å¾—å®æ—¶å“åº”ï¼š

```python
response = client.chat.completions.create(
    model="deepseek-audit",
    messages=[...],
    stream=True  # å¯ç”¨æµå¼è¾“å‡º
)

print("å®¡è®¡æŠ¥å‘Šï¼š")
for chunk in response:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### å¤šè½®å¯¹è¯

æ·±å…¥åˆ†æç‰¹å®šé—®é¢˜ï¼š

```python
messages = [
    {"role": "system", "content": "ä½ æ˜¯å®‰å…¨å®¡è®¡ä¸“å®¶"},
    {"role": "user", "content": "å®¡è®¡è¿™æ®µä»£ç ï¼š\n" + code}
]

# ç¬¬ä¸€è½®
response1 = client.chat.completions.create(
    model="deepseek-audit",
    messages=messages
)
messages.append({
    "role": "assistant",
    "content": response1.choices[0].message.content
})

# è¿½é—®
messages.append({
    "role": "user",
    "content": "è¯·è¯¦ç»†è§£é‡Šç¬¬ä¸€ä¸ªæ¼æ´çš„æ”»å‡»åœºæ™¯"
})

response2 = client.chat.completions.create(
    model="deepseek-audit",
    messages=messages
)
print(response2.choices[0].message.content)
```

## ğŸ” å®¡è®¡æŠ¥å‘Šè§£è¯»

### é£é™©ç­‰çº§

- **ä¸¥é‡**: å¯ç›´æ¥å¯¼è‡´èµ„é‡‘æŸå¤±çš„æ¼æ´ï¼Œå¿…é¡»ç«‹å³ä¿®å¤
- **é«˜å±**: å¯èƒ½å¯¼è‡´ä¸¥é‡åæœï¼Œå¼ºçƒˆå»ºè®®ä¿®å¤
- **ä¸­å±**: å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œå»ºè®®ä¿®å¤
- **ä½å±**: æœ€ä½³å®è·µå»ºè®®ï¼Œå¯é€‰ä¿®å¤

### æŠ¥å‘Šç»“æ„

å…¸å‹çš„å®¡è®¡æŠ¥å‘ŠåŒ…å«ï¼š

1. **é£é™©ç­‰çº§**: æ€»ä½“é£é™©è¯„ä¼°
2. **æ¼æ´åˆ—è¡¨**: å‘ç°çš„æ‰€æœ‰é—®é¢˜
   - æ¼æ´åç§°
   - ä¸¥é‡ç¨‹åº¦
   - æ¼æ´æè¿°
   - æ”»å‡»åŸç†
   - ä¿®å¤å»ºè®®
   - ä¿®å¤ä»£ç 
3. **æ€»ä½“è¯„ä¼°**: ç»¼åˆè¯„ä»·å’Œå»ºè®®

### å¸¸è§æ¼æ´ç±»å‹

| æ¼æ´ç±»å‹ | è¯´æ˜ | å…¸å‹åœºæ™¯ |
|---------|------|---------|
| Signer æ£€æŸ¥ç¼ºå¤± | æœªéªŒè¯è´¦æˆ·ç­¾åè€… | ä»»ä½•äººéƒ½èƒ½æ“ä½œä»–äººè´¦æˆ· |
| æ‰€æœ‰æƒéªŒè¯ç¼ºå¤± | æœªæ£€æŸ¥è´¦æˆ·å½’å± | PDA éªŒè¯ä¸å½“ |
| æ•´æ•°æº¢å‡º | ç®—æœ¯è¿ç®—æº¢å‡º | ä½™é¢è®¡ç®—é”™è¯¯ |
| é‡å…¥æ”»å‡» | è·¨ç¨‹åºè°ƒç”¨æ¼æ´ | é€’å½’è°ƒç”¨å¯¼è‡´çŠ¶æ€é”™è¯¯ |
| é€»è¾‘é”™è¯¯ | ä¸šåŠ¡é€»è¾‘ç¼ºé™· | æƒé™æ§åˆ¶ç»•è¿‡ |

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜ï¼šå®¡è®¡é€Ÿåº¦å¾ˆæ…¢

**åŸå› **: GPU æ˜¾å­˜ä¸è¶³æˆ–æ¨¡å‹æœªå……åˆ†åˆ©ç”¨ GPU

**è§£å†³**:
```bash
# è°ƒæ•´æ˜¾å­˜åˆ©ç”¨ç‡
vllm serve ... --gpu-memory-utilization 0.95

# ä½¿ç”¨é‡åŒ–æ¨¡å‹
vllm serve ... --quantization awq
```

### é—®é¢˜ï¼šç”Ÿæˆå†…å®¹è´¨é‡ä¸ä½³

**åŸå› **: Temperature è®¾ç½®ä¸å½“æˆ–æç¤ºè¯ä¸å¤Ÿæ˜ç¡®

**è§£å†³**:
```python
# é™ä½ temperature ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
response = client.chat.completions.create(
    model="deepseek-audit",
    messages=[...],
    temperature=0.1  # æ¨èèŒƒå›´ 0.1-0.3
)
```

### é—®é¢˜ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡

**è§£å†³**:
```bash
# æ£€æŸ¥ vLLM æ˜¯å¦è¿è¡Œ
curl http://localhost:8000/health

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tuln | grep 8000

# æŸ¥çœ‹ vLLM æ—¥å¿—
# (æŸ¥çœ‹å¯åŠ¨ vLLM çš„ç»ˆç«¯è¾“å‡º)
```

### é—®é¢˜ï¼šæ˜¾å­˜æº¢å‡º

**è§£å†³**:
```bash
# æ–¹æ³• 1: å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
vllm serve ... --max-model-len 4096

# æ–¹æ³• 2: å¯ç”¨é‡åŒ–
vllm serve ... --quantization awq

# æ–¹æ³• 3: é™ä½æ˜¾å­˜åˆ©ç”¨ç‡
vllm serve ... --gpu-memory-utilization 0.7
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å•æ¬¡å®¡è®¡ä¼˜åŒ–

- ä½¿ç”¨ BF16 ç²¾åº¦ï¼ˆé»˜è®¤ï¼‰
- è®¾ç½®åˆé€‚çš„ max_tokensï¼ˆæ¨è 1024-2048ï¼‰
- é™ä½ temperatureï¼ˆ0.1-0.3ï¼‰

### æ‰¹é‡å®¡è®¡ä¼˜åŒ–

- ä½¿ç”¨å¼‚æ­¥ API å¹¶å‘å¤„ç†
- åˆç†è®¾ç½®è¯·æ±‚é—´éš”
- ç›‘æ§ GPU æ˜¾å­˜ä½¿ç”¨

### æœåŠ¡å™¨é…ç½®

```bash
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --dtype bfloat16 \
    --port 8000 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --model-name deepseek-audit \
    --disable-log-requests \  # ç¦ç”¨è¯·æ±‚æ—¥å¿—
    --trust-remote-code
```

## ğŸ”— æ›´å¤šèµ„æº

- [API æ–‡æ¡£](api.md) - è¯¦ç»†çš„ API è¯´æ˜
- [ç¤ºä¾‹ä»£ç ](../examples/) - æ›´å¤šä½¿ç”¨ç¤ºä¾‹
- [å¸¸è§é—®é¢˜](deployment.md#å¸¸è§é—®é¢˜è§£ç­”) - FAQ

## ğŸ’¬ è·å–å¸®åŠ©

- ğŸ“– æŸ¥çœ‹[æ–‡æ¡£ç´¢å¼•](README.md)
- ğŸ› æäº¤ [Issue](https://github.com/Xcoconut114514/RustSentinel-of-ShanghaiOpen/issues)
- ğŸ“§ é‚®ä»¶è”ç³»: 2819404727@qq.com

---

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸ‰
